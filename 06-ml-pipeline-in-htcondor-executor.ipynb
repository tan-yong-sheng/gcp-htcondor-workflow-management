{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8571181-862a-42ee-8167-5accdac50670",
   "metadata": {},
   "source": [
    "## Workflow Management for Machine learning pipeline in HTCondor Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d75551-5695-48ec-8c8a-ad321f079aac",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "- Make sure all those Condor Executors have `pandas` and `scikit-learn` Python library installed via `sudo pip3 installl pandas scikit-learn`.\n",
    "- Install NFS server on Condor Submit, and mount it to Condor Executors\n",
    "- For your Submit Condor and Condor Executors, you need to change the ownership and permission for your home directory and NFS directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813238fe-a0fa-424b-95ff-20bdd50058d7",
   "metadata": {},
   "source": [
    "### Running `scikit-learn` in HTCondor Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86672b00-bf1b-4e34-b4b7-3a797122f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libraries\n",
    "import os\n",
    "import htcondor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98138d58-86a9-47a9-ae91-edc073e4dc2c",
   "metadata": {},
   "source": [
    "## Part 1: Load data from NFS + Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41861d0b-857a-4aad-a214-c2992b5c21f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/scripts/data_processing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/scripts/data_processing.py\n",
    "#!/usr/bin/env python3\n",
    "print(\"------------\")\n",
    "print(\"06-ml-pipeline-in-htcondor-executor.ipynb\")\n",
    "print(\"Task description: Load data from NFS and perform data pre-processing\")\n",
    "print(\"------------\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"/home/tanyongsheng_net/data\")\n",
    "staging_dir = base_dir / \"staging\"\n",
    "split_data_dir = staging_dir / \"split_data\"\n",
    "CSV_file = base_dir / \"loan_data.csv\"\n",
    "\n",
    "# Create necessary filepath directories\n",
    "os.makedirs(staging_dir, exist_ok=True)\n",
    "os.makedirs(split_data_dir, exist_ok=True)\n",
    "\n",
    "# Part 1: load data from NFS\n",
    "print(\"Loading data from NFS...\")\n",
    "data = pd.read_csv(CSV_file)\n",
    "\n",
    "# Part 2: Data Preprocessing\n",
    "## 2.1 Handling Missing Values\n",
    "  ## To identify all the numeric columns in the dataset, compute the mean value of each column,\n",
    "  ## and fill the missing values (NaN) in each column with the computed mean value.\n",
    "print(\"Handling missing values...\")\n",
    "numeric_columns = data.select_dtypes(include=['number']).columns\n",
    "for col in numeric_columns:\n",
    "    mean_value = data[col].mean()\n",
    "    data[col] = data[col].fillna(mean_value)\n",
    "\n",
    "## 2.2 Encoding Categorical Variables\n",
    "print(\"Encoding categorical variables...\")\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "## Part 3: Export cleaned data\n",
    "print(\"Saving processed data to NFS disk...\")\n",
    "data.to_csv(split_data_dir / \"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accc26d2-2198-4de6-9703-0147b5b4183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the python script is executable\n",
    "!chmod 764 ./data/scripts/data_processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af496249-0d0b-4506-aa46-03e84c1f6d76",
   "metadata": {},
   "source": [
    "## Part 2: Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ef091e-a05f-47e5-b129-0303e430c882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/scripts/train_test_split_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/scripts/train_test_split_data.py\n",
    "#!/usr/bin/env python3\n",
    "print(\"------------\")\n",
    "print(\"06-ml-pipeline-in-htcondor-executor.ipynb\")\n",
    "print(\"Task description: Perform train test split\")\n",
    "print(\"------------\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_dir = Path(\"/home/tanyongsheng_net/data\")\n",
    "staging_dir = base_dir / \"staging\"\n",
    "split_data_dir = staging_dir / \"split_data\"\n",
    "\n",
    "# Create necessary filepath directories\n",
    "os.makedirs(staging_dir, exist_ok=True)\n",
    "os.makedirs(split_data_dir, exist_ok=True)\n",
    "\n",
    "data = pd.read_csv(split_data_dir / \"processed_data.csv\")\n",
    "\n",
    "## Define the target and features\n",
    "X = data.drop('loan_status', axis=1)  # 'loan_status' is the target column\n",
    "y = data['loan_status']\n",
    "\n",
    "# Part 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "## Export train test split data\n",
    "X_train.to_csv(split_data_dir / \"X_train.csv\", index=False)\n",
    "y_train.to_csv(split_data_dir / \"y_train.csv\", index=False)\n",
    "X_test.to_csv(split_data_dir / \"X_test.csv\", index=False)\n",
    "y_test.to_csv(split_data_dir / \"y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86cf3048-ad72-4d07-a757-39ea24b8452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the python script is executable\n",
    "!chmod 764 ./data/scripts/train_test_split_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f135dd2-7da2-4919-9ab9-a4a0aad771ca",
   "metadata": {},
   "source": [
    "## Part 3: Modeling of 1st model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404ed942-15cd-4510-8726-b3c888f69da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/scripts/data_modeling_v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/scripts/data_modeling_v1.py\n",
    "#!/usr/bin/env python3\n",
    "print(\"------------\")\n",
    "print(\"06-ml-pipeline-in-htcondor-executor.ipynb\")\n",
    "print(\"Task description: Modeling with Logistic Regression for Loan Prediction\")\n",
    "print(\"------------\")\n",
    "\n",
    "## Objective: Train our previous best model for deployment\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_dir = Path(\"/home/tanyongsheng_net/data\")\n",
    "staging_dir = base_dir / \"staging\"\n",
    "split_data_dir = staging_dir / \"split_data\"\n",
    "staging_model_dir = staging_dir / \"model\"\n",
    "\n",
    "# Create necessary filepath directories\n",
    "os.makedirs(staging_model_dir, exist_ok=True)\n",
    "\n",
    "# Load previously splitted train data\n",
    "print(\"Loading training data...\")\n",
    "X_train = pd.read_csv(split_data_dir / \"X_train.csv\")\n",
    "y_train = pd.read_csv(split_data_dir / \"y_train.csv\").squeeze()\n",
    "\n",
    "# Define a pipeline for modeling\n",
    "print(\"Defining the model pipeline...\")\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Standardize the numerical data via z-score normalization\n",
    "    ('model', LogisticRegression()) # Step 2: Train the model\n",
    "])\n",
    "## Fit the pipeline to the training data\n",
    "print(\"Performing modeling...\")\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Export the trained model\n",
    "print(\"Saving the trained model...\")\n",
    "pickle.dump(pipe, open(staging_model_dir / 'trained_model_v1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63847654-806c-40f2-b702-6c19d2cfd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the python script is executable\n",
    "!chmod 764 ./data/scripts/data_modeling_v1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab410ecd-2b5e-46f7-9640-964c385611ba",
   "metadata": {},
   "source": [
    "## Part 4: Modeling of 2nd model - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4c7bde-6f9b-4df7-9917-52d8560eb6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/scripts/data_modeling_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/scripts/data_modeling_v2.py\n",
    "#!/usr/bin/env python3\n",
    "print(\"------------\")\n",
    "print(\"06-ml-pipeline-in-htcondor-executor.ipynb\")\n",
    "print(\"Task description: Decision tree for Loan Prediction\")\n",
    "print(\"------------\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base_dir = Path(\"/home/tanyongsheng_net/data\")\n",
    "staging_dir = base_dir / \"staging\"\n",
    "split_data_dir = staging_dir / \"split_data\"\n",
    "staging_model_dir = staging_dir / \"model\"\n",
    "\n",
    "# Create necessary filepath directories for data export\n",
    "os.makedirs(staging_model_dir, exist_ok=True)\n",
    "\n",
    "# Load previously splitted train data\n",
    "print(\"Loading training data...\")\n",
    "X_train = pd.read_csv(split_data_dir / \"X_train.csv\")\n",
    "y_train = pd.read_csv(split_data_dir / \"y_train.csv\").squeeze()\n",
    "\n",
    "# Define a pipeline for modeling\n",
    "print(\"Defining the model pipeline...\")\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Standardize the numerical data via z-score normalization\n",
    "    ('model', DecisionTreeClassifier()) # Step 2: Train the model\n",
    "])\n",
    "\n",
    "## Fit the pipeline to the training data\n",
    "print(\"Performing modeling...\")\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Export the trained model\n",
    "print(\"Saving the model v2...\")\n",
    "pickle.dump(pipe, open(staging_model_dir / 'trained_model_v2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59604f25-5863-441e-8e28-f3e547d0960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the python script is executable\n",
    "!chmod 764 ./data/scripts/data_modeling_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fa345-72d1-4f12-8526-79b858cc2d76",
   "metadata": {},
   "source": [
    "## Part 5a: Evaluate trained model Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77194a51-230d-4a56-919f-0e0b2aba1966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/scripts/evaluate_model_v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/scripts/evaluate_model_v1.py\n",
    "#!/usr/bin/env python3\n",
    "print(\"------------\")\n",
    "print(\"06-ml-pipeline-in-htcondor-executor.ipynb\")\n",
    "print(\"Task description: Evaluate the model v1 with test data\")\n",
    "print(\"------------\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                            precision_score, \n",
    "                            recall_score, \n",
    "                            f1_score)\n",
    "\n",
    "base_dir = Path(\"/home/tanyongsheng_net/data\")\n",
    "staging_dir = base_dir / \"staging\"\n",
    "split_data_dir = staging_dir / \"split_data\"\n",
    "metrics_dir = staging_dir / \"metrics\"\n",
    "staging_model_dir = staging_dir / \"model\"\n",
    "metrics_file = metrics_dir / \"model_metrics_v1.json\"\n",
    "\n",
    "# Ensure the folders exist\n",
    "print(\"Creating necessary directories...\")\n",
    "os.makedirs(staging_dir, exist_ok=True)\n",
    "os.makedirs(split_data_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "os.makedirs(staging_model_dir, exist_ok=True)\n",
    "\n",
    "## Load the trained model\n",
    "print(\"Loading the 1st trained model...\")\n",
    "pipe = pickle.load(open(staging_model_dir / 'trained_model_v1.pkl', 'rb'))\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load the test data\n",
    "print(\"Loading the test data...\")\n",
    "X_test = pd.read_csv(split_data_dir / \"X_test.csv\")\n",
    "y_test = pd.read_csv(split_data_dir / \"y_test.csv\")\n",
    "print(f\"Test data loaded: {X_test.shape[0]} samples\")\n",
    "\n",
    "\n",
    "## Make predictions using the pipeline\n",
    "print(\"Making predictions using the trained model...\")\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"Predictions made!\")\n",
    "\n",
    "## Calculate multiple metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')  # 'binary' for binary classification\n",
    "recall = recall_score(y_test, y_pred, average='binary')       # 'binary' for binary classification\n",
    "f1 = f1_score(y_test, y_pred, average='binary')               # 'binary' for binary classification\n",
    "\n",
    "## Export all metrics\n",
    "metrics = {\n",
    "    'Model name': 'Logistic Regression',\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1,\n",
    "}\n",
    "print(metrics)\n",
    "\n",
    "# Save the metrics to a JSON file\n",
    "print(\"Saving metrics to JSON file...\")\n",
    "\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(f\"Metrics saved at {metrics_dir}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eede871e-7498-44ac-930a-b8026c4341dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 764 ./data/scripts/evaluate_model_v1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8aca20-7021-4e17-9210-fbda3a79f5b4",
   "metadata": {},
   "source": [
    "## Part 5b: Evaluate Trained Model Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61788e26-4ade-421c-b8b3-ee2eef22b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/scripts/evaluate_model_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/scripts/evaluate_model_v2.py\n",
    "#!/usr/bin/env python3\n",
    "print(\"------------\")\n",
    "print(\"06-ml-pipeline-in-htcondor-executor.ipynb\")\n",
    "print(\"Task description: Evaluate the model v2 with test data\")\n",
    "print(\"------------\")\n",
    "\n",
    "## 6.2 Metrics of model v2\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                            precision_score, \n",
    "                            recall_score, \n",
    "                            f1_score)\n",
    "\n",
    "base_dir = Path(\"/home/tanyongsheng_net/data\")\n",
    "staging_dir = base_dir / \"staging\"\n",
    "split_data_dir = staging_dir / \"split_data\"\n",
    "metrics_dir = staging_dir / \"metrics\"\n",
    "staging_model_dir = staging_dir / \"model\"\n",
    "metrics_file = metrics_dir / \"model_metrics_v2.json\"\n",
    "\n",
    "# Ensure the folders exist\n",
    "print(\"Creating necessary directories...\")\n",
    "os.makedirs(staging_dir, exist_ok=True)\n",
    "os.makedirs(split_data_dir, exist_ok=True)\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "os.makedirs(staging_model_dir, exist_ok=True)\n",
    "\n",
    "## Load the trained model\n",
    "print(\"Loading the 1st trained model...\")\n",
    "pipe = pickle.load(open(staging_model_dir / 'trained_model_v2.pkl', 'rb'))\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load the test data\n",
    "print(\"Loading the test data...\")\n",
    "X_test = pd.read_csv(split_data_dir / \"X_test.csv\")\n",
    "y_test = pd.read_csv(split_data_dir / \"y_test.csv\")\n",
    "print(f\"Test data loaded: {X_test.shape[0]} samples\")\n",
    "\n",
    "\n",
    "## Make predictions using the pipeline\n",
    "print(\"Making predictions using the trained model...\")\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"Predictions made!\")\n",
    "\n",
    "## Calculate multiple metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')  # 'binary' for binary classification\n",
    "recall = recall_score(y_test, y_pred, average='binary')       # 'binary' for binary classification\n",
    "f1 = f1_score(y_test, y_pred, average='binary')               # 'binary' for binary classification\n",
    "\n",
    "## Export all metrics\n",
    "metrics = {\n",
    "    'Model name': 'Decision Tree',\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1,\n",
    "}\n",
    "print(metrics)\n",
    "\n",
    "# Save the metrics to a JSON file\n",
    "print(\"Saving metrics to JSON file...\")\n",
    "\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "print(f\"Metrics saved at {metrics_dir}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24e2b60-3027-4bb7-a6c9-ad099a28f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 764 ./data/scripts/evaluate_model_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81b740-7b19-4ea8-ac2e-f9f0476188fc",
   "metadata": {},
   "source": [
    "## Part 6: Deploy best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca9164db-e43b-4b20-afb2-88e8ca6c9a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/scripts/deploy_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/scripts/deploy_model.py\n",
    "#!/usr/bin/env python3\n",
    "print(\"------------\")\n",
    "print(\"06-ml-pipeline-in-htcondor-executor.ipynb\")\n",
    "print(\"Task description: Deploy best model for production\")\n",
    "print(\"------------\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"/home/tanyongsheng_net/data\")\n",
    "staging_dir = base_dir / \"staging\" \n",
    "model_dir = base_dir / \"model\"\n",
    "staging_model_dir = staging_dir / \"model\"\n",
    "metrics_dir = staging_dir / \"metrics\"\n",
    "\n",
    "# Create necessary folder path\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "print(f\"Model directory created or already exists: {model_dir}\")\n",
    "\n",
    "# Read metrics file for both trained models\n",
    "# Read metrics files for both trained models\n",
    "print(\"Loading metrics for trained models...\")\n",
    "with open(metrics_dir / \"model_metrics_v1.json\", 'r') as f:\n",
    "    trained_metrics_v1 = json.load(f)\n",
    "with open(metrics_dir / \"model_metrics_v2.json\", 'r') as f:\n",
    "    trained_metrics_v2 = json.load(f)\n",
    "\n",
    "# deploy model with higher precision value\n",
    "if trained_metrics_v1[\"Precision\"] > trained_metrics_v2[\"Precision\"]:\n",
    "    model_to_deploy = \"trained_model_v1.pkl\"\n",
    "    print(\"Deployed model v1 based on higher precision.\")\n",
    "elif trained_metrics_v1[\"Precision\"] < trained_metrics_v2[\"Precision\"]:\n",
    "    model_to_deploy = \"trained_model_v2.pkl\"\n",
    "    print(\"Deployed model v2 based on higher precision.\")\n",
    "else:\n",
    "    if trained_metrics_v1[\"F1 Score\"] >= trained_metrics_v2[\"F1 Score\"]:\n",
    "        model_to_deploy = \"trained_model_v1.pkl\"\n",
    "        print(\"Deployed model v1 based on higher F1 score as a tie-breaker.\")\n",
    "    else:\n",
    "        model_to_deploy = \"trained_model_v2.pkl\"\n",
    "        print(\"Deployed model v2 based on higher F1 score as a tie-breaker.\")\n",
    "\n",
    "# Copy the selected model to the production directory\n",
    "print(f\"Copying {model_to_deploy} to the production model directory...\")\n",
    "shutil.copy(staging_model_dir / model_to_deploy, model_dir / \"deployed_model.pkl\")\n",
    "print(f\"Deployed model saved as 'deployed_model.pkl' in {model_dir}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc3f98d2-7af6-4321-a81d-ea6e54c67f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 764 ./data/scripts/deploy_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1c1e3-b3be-4fa5-bfb0-5ce902179bac",
   "metadata": {},
   "source": [
    "## Part 7: Predict with deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd3d5ee0-ba3a-4126-8297-fdae6bc444cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/scripts/predict_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/scripts/predict_model.py\n",
    "#!/usr/bin/env python3\n",
    "print(\"------------\")\n",
    "print(\"06-ml-pipeline-in-htcondor-executor.ipynb\")\n",
    "print(\"Task description: Predict with deployed model\")\n",
    "print(\"------------\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir = Path(\"/home/tanyongsheng_net/data\")\n",
    "model_dir = base_dir / \"model\"\n",
    "staging_dir = base_dir / \"staging\" \n",
    "split_data_dir = staging_dir / \"split_data\"\n",
    "\n",
    "# Ensure the test data files exist\n",
    "if not split_data_dir.exists():\n",
    "    print(f\"Error: {split_data_dir} does not exist.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Loading the test data...\")\n",
    "X_test = pd.read_csv(split_data_dir / \"X_test.csv\")\n",
    "y_test = pd.read_csv(split_data_dir / \"y_test.csv\")\n",
    "\n",
    "# Load the saved model\n",
    "print(\"Loading the deployed model...\")\n",
    "with open(model_dir / 'deployed_model.pkl', 'rb') as f:\n",
    "    deployed_model = pickle.load(f)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "y_pred = deployed_model.predict(X_test)\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "\n",
    "print(\"Task completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "412fe9dd-ed0c-40f8-9f4d-31180dea4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 764 ./data/scripts/predict_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cf5607-9d71-42dc-8be0-2fd607585bd3",
   "metadata": {},
   "source": [
    "## Part 8: Create DAG file HTCondor Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8916fd57-446b-4f87-8970-3707fe5e1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import htcondor\n",
    "import os\n",
    "from htcondor import dags\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "base_dir = Path(\"/home/tanyongsheng_net/data\")\n",
    "\n",
    "# Function wrapper to create submit file\n",
    "def create_submit_file(task_name, request_cpus=1, request_memory=\"128MB\", request_disk=\"128MB\", **kwargs):\n",
    "    sub = htcondor.Submit({\n",
    "        \"executable\": base_dir / f\"scripts/{task_name}.py\",  # Use bash to execute shell commands\n",
    "        \"request_cpus\": request_cpus,            # Number of CPU cores required\n",
    "        \"request_memory\": request_memory,      # Memory required\n",
    "        \"request_disk\": request_disk,        # Disk space required\n",
    "        \"output\": base_dir / f\"output/{task_name}.out\",  # Standard output file\n",
    "        \"error\": base_dir / f\"error/{task_name}.err\",    # Standard error file\n",
    "        \"log\": base_dir / f\"log/{task_name}.log\",        # Log file\n",
    "    })\n",
    "    # Update default parameters with any additional kwargs\n",
    "    sub.update(kwargs)\n",
    "    return sub\n",
    "\n",
    "data_processing_sub = create_submit_file(task_name=\"data_processing\")\n",
    "train_test_split_sub = create_submit_file(task_name=\"train_test_split_data\")\n",
    "data_modeling_v1_sub = create_submit_file(task_name=\"data_modeling_v1\", request_memory=\"200MB\", request_disk=\"128MB\")\n",
    "data_modeling_v2_sub = create_submit_file(task_name=\"data_modeling_v2\", request_memory=\"200MB\", request_disk=\"128MB\")\n",
    "\n",
    "evaluate_model_v1_sub = create_submit_file(task_name=\"evaluate_model_v1\")\n",
    "evaluate_model_v2_sub = create_submit_file(task_name=\"evaluate_model_v2\")\n",
    "deploy_model_sub = create_submit_file(task_name=\"deploy_model\")\n",
    "predict_model_sub = create_submit_file(task_name=\"predict_model\")\n",
    "\n",
    "\n",
    "def create_dag():\n",
    "    # Initialize the DAG\n",
    "    dag = dags.DAG()\n",
    "\n",
    "    # Define job layers for each task\n",
    "    ## Task 1: data processing\n",
    "    data_processing_layer = dag.layer(\n",
    "        name=\"data_processing\",\n",
    "        submit_description=data_processing_sub\n",
    "    )\n",
    "\n",
    "    ## Task 2: train test split\n",
    "    train_test_split_layer = dag.layer(\n",
    "        name=\"train_test_split\",\n",
    "        submit_description=train_test_split_sub \n",
    "    )\n",
    "    train_test_split_layer.add_parents([data_processing_layer])\n",
    "    \n",
    "    ## Data modeling v1 and v2 concurrently\n",
    "    ## Task 3: Data modeling v1\n",
    "    data_modeling_v1_layer = dag.layer(\n",
    "        name=\"modeling_v1\",\n",
    "        submit_description=data_modeling_v1_sub\n",
    "    )\n",
    "    data_modeling_v1_layer.add_parents([train_test_split_layer])\n",
    "\n",
    "    # Task 4: Data modeling v2\n",
    "    data_modeling_v2_layer = dag.layer(\n",
    "        name=\"modeling_v2\",\n",
    "        submit_description=data_modeling_v2_sub\n",
    "    )\n",
    "    data_modeling_v2_layer.add_parents([train_test_split_layer])\n",
    "\n",
    "    # Task 5: Evaluate trained model v1\n",
    "    evaluate_model_v1_layer = dag.layer(\n",
    "        name=\"evaluate_model_v1\",\n",
    "        submit_description=evaluate_model_v1_sub\n",
    "    )\n",
    "    evaluate_model_v1_layer.add_parents([data_modeling_v1_layer])\n",
    "\n",
    "    # Task 6: Evaluate trained model v2\n",
    "    evaluate_model_v2_layer = dag.layer(\n",
    "        name=\"evaluate_model_v2\",\n",
    "        submit_description=evaluate_model_v2_sub\n",
    "    )\n",
    "    evaluate_model_v2_layer.add_parents([data_modeling_v2_layer])\n",
    "\n",
    "    # Task 7: Deploy best model\n",
    "    deploy_model_layer = dag.layer(\n",
    "        name=\"deploy_best_model\",\n",
    "        submit_description=deploy_model_sub\n",
    "    )\n",
    "    deploy_model_layer.add_parents([evaluate_model_v1_layer, evaluate_model_v2_layer])\n",
    "\n",
    "    # Task 8: Predict with best model\n",
    "    predict_model_layer = dag.layer(\n",
    "        name=\"loan_prediction\",\n",
    "        submit_description=predict_model_sub\n",
    "    )\n",
    "    predict_model_layer.add_parents([deploy_model_layer])\n",
    "    return dag\n",
    "\n",
    "dag = create_dag()\n",
    "\n",
    "# Set up the DAG directory\n",
    "# Write the DAG to disk\n",
    "dag_dir = os.path.abspath(\"./data/dags/\")\n",
    "os.makedirs(dag_dir, exist_ok=True)\n",
    "dag_file = dags.write_dag(dag, dag_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51be0a-cef1-41e1-9a66-12503ed890a4",
   "metadata": {},
   "source": [
    "## Part 9: Submit the Job via DAG file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a48034-0abf-4c3c-9ca0-d2f5109eea7f",
   "metadata": {},
   "source": [
    "Go to the terminal\n",
    "\n",
    "> cd ~/data/dags\n",
    "\n",
    "> condor_submit_dag -f dagfile.dag\n",
    "\n",
    "> watch -n 1 condor_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9deed6-074b-4ed6-8a85-31d5b1cf7160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
